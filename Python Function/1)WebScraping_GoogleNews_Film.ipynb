{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9KQlb5Gzjz6"
      },
      "source": [
        "# Codice scraping da google news "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Installazione librerie"
      ],
      "metadata": {
        "id": "JK4G3_p8hQYM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kMrcbHRZz6yE"
      },
      "outputs": [],
      "source": [
        "pip install requests_html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pFBwCBZmz6c7"
      },
      "outputs": [],
      "source": [
        "pip install pygooglenews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PBHinrv0-b8"
      },
      "source": [
        "## Funzione di scraping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fr2fdLZjuY-"
      },
      "outputs": [],
      "source": [
        "# Importiamo le librerie\n",
        "import pandas as pd\n",
        "from pygooglenews import GoogleNews                           \n",
        "from datetime import datetime, timedelta\n",
        "import urllib.request\n",
        "from lxml.html import fromstring\n",
        "import requests\n",
        "import re\n",
        "\n",
        "# Definiamo una funzione le cui variabili sono: search (parole chiave della ricerca), start_date (data inizio ricerca), end_date (data fine ricerca)\n",
        "def get_storyII(search, start_date, end_date):\n",
        "  # Definiamo story e df_stories come variabili globali\n",
        "  global story\n",
        "  global df_stories\n",
        "  # Settiamo le impostazioni della ricerca: lingua inglese (lang), frequenza giornaliera (timedelta), insieme di date (data_list)\n",
        "  gn = GoogleNews(lang = 'en')\n",
        "  delta = timedelta(days = 1)  \n",
        "  date_list = pd.date_range(start_date, end_date).tolist()  \n",
        "  stories = []\n",
        "  \n",
        "  HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5)'\n",
        "                  ' AppleWebKit/537.36 (KHTML, like Gecko) Cafari/537.36'\n",
        "             }\n",
        "  # Per ogni giorno: cerca le notizie su google_news (result['entries'])\n",
        "  for date in date_list[:-1]:   #helper = True--> ottimizza la ricerca delle parole chiave \n",
        "      result = gn.search(query = search, helper = True, from_=date.strftime('%Y-%m-%d'), to_=(date+delta).strftime('%Y-%m-%d'))   #seleziona pagina google news in un dato giorno\n",
        "      newsitem = result['entries']   #riporta tutti gli oggetti della pagina google in un dato giorno (così per ogni giorno nel range)\n",
        "\n",
        "      \n",
        "      for item in newsitem:    # Apri ogni articolo e ricerca: titolo, link, data e twitter_link (se esistente)           \n",
        "        try:\n",
        "            # Verifica che venga aperto l'articolo\n",
        "          print(item.link, item.published[4:16])   \n",
        "          r = requests.get(item.link, headers=HEADERS, allow_redirects=False)    # apri la pagina dell'articolo\n",
        "          tree = fromstring(r.content)                        # mappa la struttura della pagina HTML\n",
        "          try:\n",
        "            # trovi tutti i link nella pagina\n",
        "            links = tree.xpath('//a/@href')                     \n",
        "            f = re.compile('(https://twitter.com)/(\\w+)/(status)/(\\d+)')                \n",
        "            # se trovi uno o più twitter_link, aggiungili ad una lista \n",
        "            twitter_links = [f.search(link).group() for link in links if f.search(link)]\n",
        "            twitter_id = [f.search(link).group(4) for link in links if f.search(link)]\n",
        "            story = {\n",
        "                  'title':item.title,\n",
        "                  'link':item.link,\n",
        "                  'published':item.published[4:16],\n",
        "                  'twitter_links': twitter_links,\n",
        "                  'twitter_id': twitter_id  \n",
        "            } \n",
        "            stories.append(story)\n",
        "          except:\n",
        "            continue\n",
        "        except:\n",
        "          continue  \n",
        "      df_stories = pd.DataFrame(stories)\n",
        "  return(df_stories)\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxgwu4WM_Soe",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "#prova\n",
        "get_storyII(\"Iron Man\", '2008-1-26','2008-6-2')\n",
        "#\"Iron Man\": [ '2008-1-26', '2008-6-2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mi8hntaZlgi5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "film_title = {'Phase 1': {\"Iron Man\": [ '2008-1-26', '2008-6-2'], \n",
        "                          \"The Incredible Hulk\": ['2008-02-01', '2008-07-13'],\n",
        "                          \"Iron Man 2\": [ '2009-07-23', '2010-06-07'], \n",
        "                          \"Thor\": ['2010-12-09', '2011-06-06'],\n",
        "                          \"Captain America: The First Avenger\": ['2011-03-21', '2011-08-22'],\n",
        "                          \"The Avengers\": ['2011-10-11', '2012-06-04']},\n",
        "                'Phase 2': {\"Iron Man 3\": [ '2012-10-23', '2013-05-14'], \n",
        "                            \"Thor: The Dark World\": ['2013-04-24', '2013-11-22'],\n",
        "                            \"Captain America: The Winter Soldier\": [ '2013-10-25', '2014-04-13'],\n",
        "                            \"Guardians of Galaxy\": ['2014-02-18', '2014-09-01'], \n",
        "                            \"Avengers: Age of Ultron\": ['2014-10-23', '2015-06-01'],\n",
        "                            \"Ant-Man\": ['2015-04-13', '2015-08-17']},\n",
        "                'Phase 3': {\"Captain America: Civil War\": [ '2016-04-12', '2016-06-06'],\n",
        "                            \"Doctor Strange\": ['2016-04-13', '2016-12-04'],\n",
        "                            \"Guardians of Galaxy Vol.2\": [ '2016-12-04', '2017-06-05'],\n",
        "                            \"Spider-Man: Homecoming\": ['2016-12-11', '2017-08-07'],\n",
        "                            \"Thor: Ragnarok\": ['2017-04-10', '2017-11-10'],\n",
        "                            \"Black Panther\": ['2017-06-10', '2018-03-16'],\n",
        "                            \"Avengers: Infinity War\": ['2018-03-16', '2018-05-27'],\n",
        "                            \"Ant-Man and the Wasp\": ['2018-01-20', '2018-08-06'],\n",
        "                            \"Captain Marvel\": ['2018-09-18', '2019-04-08'],\n",
        "                            \"Avengers: Endgame\": ['2019-03-14', '2019-05-26'],\n",
        "                            \" Spider-Man: Far From Home\": ['2019-01-15', '2019-08-02']},\n",
        "                'Phase 4': {\"Black Widow\": ['2019-12-03','2021-08-09'],\n",
        "                            \"Shang-Chi and the Legend of the Ten Rings\": ['2021-4-19', '2021-10-3'],\n",
        "                            \"Eternals\":['2021-05-24', '2021-12-05'],\n",
        "                            \"Spider-Man: No Way Home\": ['2021-08-23', '2022-01-14'],\n",
        "                            \"Doctor Strange in the Multiverse of Madness\": ['2021-12-15', '2022-06-06']}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHUAE5M0jfGu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "def loop_fII():\n",
        "  # Per ogni fase e per ogni film\n",
        "  for phase in film_title:\n",
        "    for title in film_title[phase]:\n",
        "      search = '{} Twitter'.format(title)\n",
        "      start_date_input = datetime.strptime(film_title[phase][title][0], '%Y-%m-%d')\n",
        "      end_date_input = datetime.strptime(film_title[phase][title][1], '%Y-%m-%d')\n",
        "\n",
        "      # Costruisci una lista con tutti i periodi mensili tra start_date_input e end_date_input. Obbiettivo: superare il limite di 100 articoli scaricabili \n",
        "      a = ((end_date_input-start_date_input)/31).days    # numero totale di mesi\n",
        "      delta = timedelta(days= 31)\n",
        "      start_list = [start_date_input + (i*delta) for i in range(a+1)]\n",
        "      start_list.append(end_date_input)  \n",
        "\n",
        "      # Costruisci un dizionario i cui elementi sono i dataset ottenuti iterando la funzione per ogni coppia (star/end_date) in start_list \n",
        "      try:  \n",
        "        dictio = {el: get_storyII(search, start_list[el],start_list[el+1]) for el in range(len(start_list)-1)}  \n",
        "      except:\n",
        "        print('Notizie non trovate per questo film')\n",
        "        continue\n",
        "\n",
        "      # Concatena i dataset in un unico dataset\n",
        "      Scraping_news = dictio[0]\n",
        "      for x in range(1,len(dictio)):\n",
        "        # Evita che concateni dataset vuoti\n",
        "        if len(dictio[x]) != 0:\n",
        "          Scraping_news = pd.concat([Scraping_news, dictio[x]], ignore_index=True)  \n",
        "        else:\n",
        "          print('Notizie non trovate per un mese')\n",
        "          continue\n",
        "      \n",
        "        # Uniforma l'attributo 'published'\n",
        "        Scraping_news['published'] = pd.to_datetime(Scraping_news['published']) \n",
        "        Scraping_news['published'] = Scraping_news['published'].dt.strftime('%d/%m/%Y')\n",
        "        \n",
        "        # Salviamo il dataset in una cartella sul drive \n",
        "        # Scraping_news.to_csv(('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON SCRAPING TOM/{} scraping_news.csv').format(search))\n",
        "        Scraping_news.to_json(('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON SCRAPING TOM/{} scraping_news.json').format(search), orient='records')\n",
        "\n",
        "  return('Scraping completato')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IesPvqjX7P4S"
      },
      "outputs": [],
      "source": [
        "loop_fII()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "1)WebScraping_GoogleNews_Film.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}