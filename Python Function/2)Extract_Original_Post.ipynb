{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scarica tweets partendo dai 'tweet_id' contenuti negli articoli di google news. Per ogni tweet estrai nuovi 'tweet_id' dalla sezione 'urls'."
      ],
      "metadata": {
        "id": "qY5lChRMZ47l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Scarica le librerie necessarie**"
      ],
      "metadata": {
        "id": "5Y-C1rtSacd8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9ZDNLejq9AB"
      },
      "outputs": [],
      "source": [
        "pip install tweepy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OI7-8JJeouP5"
      },
      "outputs": [],
      "source": [
        "pip install requests_html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Colleghiamoci a Mydrive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Cartella da cui importare file = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON SCRAPING CLEAN TOM'   \n",
        "# Cartella in cui esportare file post twitter = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON TWITTER TOM'  \n",
        "\n",
        "# Spostati nella cartella contenente tutti i dataset di scraping\n",
        "path = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON SCRAPING CLEAN'\n",
        "os.chdir(path)\n",
        "\n",
        "# Importa tutti i file della cartella\n",
        "# scrape_list = [i for i in glob.glob('*.json')]\n",
        "scrape_list = [i for i in glob.glob('*.json')]\n",
        "\n",
        "# Inseriscili in un dizionario\n",
        "diz = {'{}'.format((m).split(' scraping')[0]): pd.read_json(m, orient='records') for m in scrape_list}"
      ],
      "metadata": {
        "id": "gq3FMVM2objm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBWoxKOpuvaI"
      },
      "outputs": [],
      "source": [
        "# Per ogni film costruisci una lista contenente tutti i 'tweet_id'\n",
        "dictio_list = {}\n",
        "for n in diz:\n",
        "  dictio_list[n] = []\n",
        "  for row in diz[n]['twitter_id'].values:\n",
        "    f = re.compile('(\\d+)') #\\d serve a prendere i numeri mentre il + le espressioni, metodo per compilare pattern di un espressione \n",
        "    for el in row:\n",
        "        m = f.search(el)\n",
        "        if m:\n",
        "          k = m.group(1)\n",
        "          if k not in dictio_list[n]: \n",
        "            dictio_list[n].append(int(k))\n",
        "          else:\n",
        "              dictio_list[n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bT9_LtbNQTwZ"
      },
      "outputs": [],
      "source": [
        "# Controlla quanti tweet_id sono presenti in ogni dataset\n",
        "for n in diz:\n",
        "  print(len(diz[n]), len(dictio_list[n]), n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMv_A7pT2k9n"
      },
      "source": [
        "##**Funzione API Twitter**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBDW0TRKDphT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tweepy\n",
        "import datetime\n",
        "import re\n",
        "def Prime(lista):\n",
        "  # Importa le credenziali fornite da Twitter per utilizzare le API\n",
        "  consumer_key = \"uBJISqAHpRkCm79dKDwsBkHL1\"\n",
        "  consumer_secret = \"NHvp1zx4Sk5n15iV4A1v2wK6Ygo7BAgheFOgEyRPmNcBkHthNq\"\n",
        "  access_key = \"1460586759918600195-SR05QtXUgVfk7WjAJFe8RcG3a22XG0\"\n",
        "  access_secret = \"MTwCZtmuHmvJqfxM943VqWq02GZvyeCj0Oflbit1w1y7n\"\n",
        "  auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "  auth.set_access_token(access_key, access_secret)  \n",
        "  api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "\n",
        "  def Post(lista):\n",
        "      # Per superare il limite di ricerca di Twitter (pari a 100 tweets), prendi gruppi da 100 elementi alla volta dalla lista dei tweet_id.\n",
        "      dictio = {}     \n",
        "      # Per coprire tutti gli elementi nella lista imposta l+1 gruppi da 100 (es: 2001 righe, round(2001/100)=round(20,01)=20 -> perdi una riga)\n",
        "      for l in range(0,(round((len(lista)/100)+0.5))):   \n",
        "          id_ = lista[100*(l):100*(l+1)]\n",
        "          tweet_obj = {} \n",
        "          # Metodo di ricerca tweets\n",
        "          tweets = api.statuses_lookup(id_, tweet_mode = 'extended', lang = 'en')  \n",
        "          for n in range(11):\n",
        "            tweet_obj[n] = []\n",
        "          for i in range(len(tweets)):\n",
        "            tweet_obj[0].append(tweets[i].user.screen_name)\n",
        "            tweet_obj[1].append(tweets[i].user.id)\n",
        "            tweet_obj[2].append(tweets[i].user.verified)\n",
        "            tweet_obj[3].append(tweets[i].user.created_at)\n",
        "            tweet_obj[4].append(str(tweets[i].id))\n",
        "            tweet_obj[5].append(pd.to_datetime(tweets[i].created_at))\n",
        "            tweet_obj[6].append(tweets[i].user.description)\n",
        "            tweet_obj[7].append(tweets[i].full_text)\n",
        "            tweet_obj[8].append(re.findall(r'#(\\w+)', tweets[i].full_text))   # estrai gli hashtags dal testo\n",
        "            tweet_obj[9].append(tweets[i].source)\n",
        "            tweet_obj[10].append(tweets[i].entities['urls'])\n",
        "\n",
        "# Costruisci il dataframe con i tweets della lista di partenza\n",
        "          f = pd.DataFrame(tweet_obj).rename(columns={0:'Username', 1:'User_id', 2:'User_verified', 3:'User_creation', 4:'Tweet_id', 5:'Date', 6:'User_Description', 7:'Text', 8:'Hashtags', 9:'Source', 10:'Urls'})    \n",
        "          f['User_creation'] = pd.to_datetime(f['User_creation']) \n",
        "          f['User_creation'] = f['User_creation'].dt.strftime('%d/%m/%Y')  # formatta attributo 'User_creation'\n",
        "          f['Date'] = pd.to_datetime(f['Date'])    # formatta attributo 'Date'\n",
        "          f['Date'] = f['Date'].dt.strftime('%d/%m/%Y')\n",
        "          dictio[l] = f\n",
        "          \n",
        "# Concatena tutti i 'l' dataframe         \n",
        "      Original_post = dictio[0]\n",
        "      for i in range(1,(round((len(lista)/100)+0.5))):\n",
        "        Original_post = pd.concat([Original_post, dictio[i]]).sort_values(by = 'Date')\n",
        "\n",
        "# Costruisci un nuovo indice          \n",
        "      z = []\n",
        "      for el in range(1,(len(Original_post)+1)):\n",
        "          z.append(el)\n",
        "\n",
        "      Original_post[\"Id\"]=z\n",
        "      Original_post.reset_index(inplace = True)\n",
        "      del[Original_post['index']]\n",
        "      # Elimina duplicati\n",
        "      Original_post = Original_post.drop_duplicates(subset =\"Tweet_id\", keep = 'first')\n",
        "      Original_post = Original_post.set_index('Id')\n",
        "      print(len(Original_post))\n",
        "      return(Original_post)   \n",
        "\n",
        "  def join_table_function(Original_post):   # costruisci una lista con tutti i tweet_id ottenuti dai link del dataframe precedente\n",
        "    a = Original_post.reset_index()    \n",
        "    b = a['Urls'].to_list()\n",
        "    t = a['Id'].to_list()\n",
        "\n",
        "    h = re.compile('(https://twitter.com)/(\\w+)/(status)/(\\d+)')\n",
        "    tweet_id = []\n",
        "    for row in range(len(a)):\n",
        "      c = str(b[row]).split(', ')   # dividi i vari gruppi di link\n",
        "      for elem in range(len(c)):\n",
        "        d = c[elem].split(': ')     # dividi i vari link\n",
        "        for x in range(len(d)):\n",
        "          m = h.search(d[x])      \n",
        "          if m:\n",
        "            tweet_id.append(m.group(4))\n",
        "\n",
        "      \n",
        "    return(tweet_id)\n",
        "    \n",
        "  post = Post(lista)     # la variabile post è il dataframe ottenuto dall'estrazione tweets (Post) dalla lista di tweet_id (lista)\n",
        "  jt = join_table_function(post)   # la variabile jt è il dataframe ottenuto dall'estrazione tweets_id (join_table_function) dal dataframe tweets (Post(lista))\n",
        "\n",
        "  while len(jt) != 0:\n",
        "    post1 = Post(jt)\n",
        "    jt = join_table_function(post1)\n",
        "    if len(post1) != 0:     # evita che la mancata presenza di tweet blocchi la funzione\n",
        "      if post.iloc[-1]['Tweet_id'] != post1.iloc[0]['Tweet_id']:    # se un tweet rimanda a se stesso, interrompi il ciclo for\n",
        "        post = pd.concat([post, post1], ignore_index = True)\n",
        "      else:\n",
        "        break\n",
        "    else:\n",
        "      break\n",
        "\n",
        "\n",
        "  post.drop_duplicates(subset = 'Tweet_id', inplace = True, keep = 'first')\n",
        "  post.to_json('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON TWITTER/Final_post_{}.json'.format(n), orient='records')\n",
        "  return(post, jt)\n",
        "\n",
        "\n",
        "for n in diz: \n",
        "  print(n)\n",
        "  if len(diz[n]) != 0:   # per ogni film\n",
        "    lista = dictio_list[n]      # La lista iniziale contiene gli id estratti dagli articoli\n",
        "    Prime(lista)\n",
        "  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "2)Extract_Original_Post.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}