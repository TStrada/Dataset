{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3)Preprocessing&Upload Mongodb.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Scarichiamo le librerie necessarie al funzionamento di pymongo**"
      ],
      "metadata": {
        "id": "OvQpryF6HHfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymongo[srv]"
      ],
      "metadata": {
        "id": "r2ELC0HAHF9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dnspython"
      ],
      "metadata": {
        "id": "FzTXCMMPHF9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymongo"
      ],
      "metadata": {
        "id": "IxBAwrKXHF9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install mongodb\n"
      ],
      "metadata": {
        "id": "ToAALXcEHF9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!service mongodb start"
      ],
      "metadata": {
        "id": "GRFa76q7HF9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Scraping news"
      ],
      "metadata": {
        "id": "aSmLIF_Jw3uI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_aOjulO2RD5"
      },
      "source": [
        "##**Elimina tuple che hanno 'twitter_link' e quindi 'twitter_id' vuote**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAA_uQrLEnc0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Colleghiamoci a Mydrive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Cartella da cui importare file(JSON) = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON SCRAPING TOM'   \n",
        "# Cartella da cui esportare file(JSON) puliti = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON SCRAPING CLEAN TOM'   \n",
        "\n",
        "# Spostati nella cartella contenente tutti i dataset di scraping\n",
        "path = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON SCRAPING'\n",
        "os.chdir(path)\n",
        "\n",
        "# Importa tutti i file della cartella\n",
        "scrape_list = [i for i in glob.glob('*.json')] #[:7], [8:14] \n",
        "\n",
        "# Inseriscili in un dizionario\n",
        "diz = {'{}'.format((m).split(' scraping')[0]): pd.read_json(m, orient='records') for m in scrape_list}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aDPaLnY7tVx"
      },
      "outputs": [],
      "source": [
        "# # Elimina righe e dataset vuoti\n",
        "for el in diz:\n",
        "  diz[el] = diz[el][diz[el]['twitter_id'].apply(lambda x: len(x)) != 0]\n",
        "  if len(diz[el]) != 0:\n",
        "    diz[el].to_json(('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON SCRAPING CLEAN TOM/{} scraping_news_clean.json').format(el), orient='records')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing MongoDB"
      ],
      "metadata": {
        "id": "9Oqx9Ph2xOXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Importa i dataset di scraping e twitter**"
      ],
      "metadata": {
        "id": "OMpYv-hNwJ3h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LLkBqNPwI29"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pymongo\n",
        "import glob\n",
        "import json\n",
        "\n",
        "# Colleghiamoci a Mydrive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Cartella in cui importare file (JSON) = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON SCRAPING CLEAN TOM'  \n",
        "# Cartella in cui esportare file post twitter (JSON) = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON TWITTER TOM'  \n",
        "\n",
        "path_scraping = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON SCRAPING CLEAN'\n",
        "path_twitter= '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON TWITTER'\n",
        "# Spostati nella cartella contenente tutti i dataset di scraping\n",
        "os.chdir(path_scraping)\n",
        "\n",
        "# Importa tutti i file della cartella \n",
        "scrape_list = [i for i in glob.glob('*.json')] \n",
        "\n",
        "# Inseriscili in un dizionario \n",
        "diz_scraping = {'{}'.format((m).split(' Twitter')[0]): json.loads(open(m).read()) for m in scrape_list}\n",
        "\n",
        "# Spostati nella cartella contenente tutti i dataset di Twitter\n",
        "os.chdir(path_twitter)\n",
        "\n",
        "# Importa tutti i file della cartella \n",
        "tw_list = [h for h in glob.glob('*.json')] #[:7], [8:14] \n",
        "\n",
        "# Inseriscili in un dizionario\n",
        "diz_tw = {'{}'.format(((n).split('Final_post_')[1]).split(' Twitter')[0]): json.loads(open(n).read()) for n in tw_list}\n",
        "\n",
        "print(tw_list)\n",
        "print(scrape_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Per ogni film creiamo il campo 'twitter id' che contenga la tupla del dataset twitter corrispondente a quell'id**"
      ],
      "metadata": {
        "id": "gDd6nJHc0lp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for film in diz_tw:\n",
        "  for row in range(len(diz_scraping[film])):\n",
        "    f = [diz_tw[film][el]     # metti il contenuto di diz_tw della riga el Se: *\n",
        "         for n in range(len(diz_scraping[film][row]['twitter_id']))   # per ogni n elemento della colonna 'twitter_id' alla riga row\n",
        "         for el in range(len(diz_tw[film]))       # per ogni riga di diz_tw \n",
        "         if diz_tw[film][el]['Tweet_id'] == diz_scraping[film][row]['twitter_id'][n]]   # * tweet_id in diz_tw == all'el-esimo twitter_id di diz_scraping     \n",
        "    diz_scraping[film][row]['twitter_id'] = f\n"
      ],
      "metadata": {
        "id": "W3P1dxlEZ6IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diz_scraping['Doctor Strange in the Multiverse of Madness'][0]"
      ],
      "metadata": {
        "id": "9iXy3TnPqkC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Caricamento documenti su database MongoDb"
      ],
      "metadata": {
        "id": "yHFLIUgVHSz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Costruiamo un documento per ogni film**"
      ],
      "metadata": {
        "id": "3KRjlIrO15go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymongo \n",
        "client = pymongo.MongoClient('mongodb+srv://TStrada:DataMan@clusterproject.laukf.mongodb.net/test')\n",
        "db = client['Film-Marvel']\n",
        "phase1 = db['Phase1']\n",
        "phase2 = db['Phase2']\n",
        "phase3 = db['Phase3']\n",
        "phase4 = db['Phase4']\n"
      ],
      "metadata": {
        "id": "T88TXtHq_Lrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phase 1**"
      ],
      "metadata": {
        "id": "wkN5hmoB9Lan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Phase 1\n",
        "d1 = {'Film': 'Iron Man', 'Release Date': '2008-05-02', 'Main Character': 'Iron Man', 'Articles and Tweets': np.nan}\n",
        "d2 = {'Film': 'The Incredible Hulk', 'Release Date': '2008-06-13', 'Main Character': 'Hulk', 'Articles and Tweets': np.nan}\n",
        "d3 = {'Film': 'Iron Man 2', 'Release Date': '2010-05-07', 'Main Character': 'Iron Man', 'Articles and Tweets': np.nan}\n",
        "d4 = {'Film': 'Thor', 'Release Date': '2011-05-06', 'Main Character': 'Thor', 'Articles and Tweets': np.nan}\n",
        "d5 = {'Film': 'Captain America: The First Avenger', 'Release Date': '2011-07-22', 'Main Character': 'Captain America', 'Articles and Tweets': np.nan}\n",
        "d6 = {'Film': 'The Avengers', 'Release Date': '2012-05-04', 'Main Character': 'Avengers', 'Articles and Tweets': np.nan}\n",
        "\n",
        "for film in diz_scraping:\n",
        "  if (d1['Film'] == film):\n",
        "    d1['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "    \n",
        "  elif (d2['Film'] == film):\n",
        "    d2['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "\n",
        "  elif( d3['Film'] == film):\n",
        "    d3['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "\n",
        "  elif(d4['Film'] == film):\n",
        "    d3['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "\n",
        "  elif(d5['Film'] == film):\n",
        "    d3['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "\n",
        "  elif(d6['Film'] == film):\n",
        "    d6['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "\n",
        "# Salva documenti\n",
        "list_doc = [d1, d2, d3, d4, d5, d6]\n",
        "for doc in list_doc:\n",
        "  with open('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/Mongodb DF (JSON)/Phase1:{}.json'.format(doc['Film']), 'w') as file:\n",
        "    json.dump(doc, file)\n",
        "  \n",
        "# Carica su mongodb\n",
        "db.phase1.insert_one(d1)   \n",
        "db.phase1.insert_one(d2)   \n",
        "db.phase1.insert_one(d3)   \n",
        "db.phase1.insert_one(d4)   \n",
        "db.phase1.insert_one(d5)   \n",
        "db.phase1.insert_one(d6)   "
      ],
      "metadata": {
        "id": "fO6pv_d-5Nnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phase 2**"
      ],
      "metadata": {
        "id": "xGAcbjrY93N_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d7 = {'Film': 'Iron Man 3', 'Release Date': '2013-04-14', 'Main Character': 'Iron Man', 'Articles and Tweets': np.nan}\n",
        "d8 = {'Film': 'Thor: The Dark World', 'Release Date': '2013-10-22', 'Main Character': 'Thor', 'Articles and Tweets': np.nan}\n",
        "d9 = {'Film': 'Captain America: The Winter Soldier', 'Release Date': '2014-03-13', 'Main Character': 'Captain America', 'Articles and Tweets': np.nan}\n",
        "d10 = {'Film': 'Guardians of Galaxy', 'Release Date': '2014-08-01', 'Main Character': 'Guardians of Galaxy', 'Articles and Tweets': np.nan}\n",
        "d11 = {'Film': 'Avengers: Age of Ultron', 'Release Date': '2015-05-01', 'Main Character': 'Avengers', 'Articles and Tweets': np.nan}\n",
        "d12 = {'Film': 'Ant-Man', 'Release Date': '2015-07-17', 'Main Character': 'Ant Man', 'Articles and Tweets': np.nan}\n",
        "\n",
        "for film in diz_scraping:\n",
        "  if (d7['Film'] == film):\n",
        "    d7['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "    \n",
        "  elif (d8['Film'] == film):\n",
        "    d8['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "\n",
        "  elif(d9['Film'] == film):\n",
        "    d9['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "\n",
        "  elif(d10['Film'] == film):\n",
        "    d10['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "\n",
        "  elif(d11['Film'] == film):\n",
        "    d11['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "\n",
        "  elif(d12['Film'] == film):\n",
        "    d12['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "\n",
        "# Salva documenti\n",
        "list_doc = [d7, d8, d9, d10, d11, d12]\n",
        "for doc in list_doc:\n",
        "  with open('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/Mongodb DF (JSON)/Phase2:{}.json'.format(doc['Film']), 'w') as file:\n",
        "    json.dump(doc, file)\n",
        "\n",
        "\n",
        "# Carica su mongodb\n",
        "db.phase2.insert_one(d7)   \n",
        "db.phase2.insert_one(d8)   \n",
        "db.phase2.insert_one(d9)   \n",
        "db.phase2.insert_one(d10)   \n",
        "db.phase2.insert_one(d11)   \n",
        "db.phase2.insert_one(d12)   "
      ],
      "metadata": {
        "id": "e732552D6O3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phase 3**"
      ],
      "metadata": {
        "id": "NlfSQZ6m-Xaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d13 = {'Film': 'Captain America: Civil War', 'Release Date': '2016-05-06', 'Main Character': 'Captain America', 'Articles and Tweets': np.nan}\n",
        "d14 = {'Film': 'Doctor Strange', 'Release Date': '2016-11-04', 'Main Character': 'Doctor Strange', 'Articles and Tweets': np.nan}\n",
        "d15 = {'Film': 'Guardians of Galaxy Vol.2', 'Release Date': '2017-05-05', 'Main Character': 'Guardians of Galaxy', 'Articles and Tweets': np.nan}\n",
        "d16 = {'Film': 'Spider-Man: Homecoming', 'Release Date': '2017-07-07', 'Main Character': 'Spider-Man', 'Articles and Tweets': np.nan}\n",
        "d17 = {'Film': 'Thor: Ragnarok', 'Release Date': '2017-10-10', 'Main Character': 'Thor', 'Articles and Tweets': np.nan}\n",
        "d18 = {'Film': 'Black Panther', 'Release Date': '2018-02-16', 'Main Character': 'Black Panther', 'Articles and Tweets': np.nan}\n",
        "d19 = {'Film': 'Avengers: Infinity War', 'Release Date': '2018-04-27', 'Main Character': 'Avengers', 'Articles and Tweets': np.nan}\n",
        "d20 = {'Film': 'Ant-Man and the Wasp', 'Release Date': '2018-07-06', 'Main Character': 'Ant Man', 'Articles and Tweets': np.nan}\n",
        "d21 = {'Film': 'Captain Marvel', 'Release Date': '2019-03-08', 'Main Character': 'Captain Marvel', 'Articles and Tweets': np.nan}\n",
        "d22 = {'Film': 'Avengers: Endgame', 'Release Date': '2019-04-26', 'Main Character': 'Avengers', 'Articles and Tweets': np.nan}\n",
        "d23 = {'Film': 'Spider-Man: Far From Home', 'Release Date': '2019-07-02', 'Main Character': 'Spider-Man', 'Articles and Tweets': np.nan}\n",
        "\n",
        "for film in diz_scraping:\n",
        "  if(d13['Film'] == film):\n",
        "    d13['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "  \n",
        "  elif(d14['Film'] == film):\n",
        "    d14['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "  \n",
        "  elif(d15['Film'] == film):\n",
        "    d15['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "  \n",
        "  elif(d16['Film'] == film):\n",
        "    d16['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "  \n",
        "  elif(d17['Film'] == film):\n",
        "    d17['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "  \n",
        "  elif(d18['Film'] == film):\n",
        "    d18['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "  \n",
        "  elif(d19['Film'] == film):\n",
        "    d19['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "  \n",
        "  elif(d20['Film'] == film):\n",
        "    d20['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "  \n",
        "  elif(d21['Film'] == film):\n",
        "    d21['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "  \n",
        "  elif(d22['Film'] == film):\n",
        "    d22['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "  \n",
        "  elif(d23['Film'] == film):\n",
        "    d23['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "\n",
        "# Salva documenti\n",
        "list_doc = [d13, d14, d15, d16, d17, d19, d20, d21, d22, d23]\n",
        "for doc in list_doc:\n",
        "  with open('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/Mongodb DF (JSON)/Phase3:{}.json'.format(doc['Film']), 'w') as file:\n",
        "    json.dump(doc, file)\n",
        "\n",
        "# Carica su mongodb\n",
        "db.phase3.insert_one(d13)   \n",
        "db.phase3.insert_one(d14)   \n",
        "db.phase3.insert_one(d15)   \n",
        "db.phase3.insert_one(d16)   \n",
        "db.phase3.insert_one(d17)   \n",
        "db.phase3.insert_one(d18) \n",
        "db.phase3.insert_one(d19)   \n",
        "db.phase3.insert_one(d20)   \n",
        "db.phase3.insert_one(d21)   \n",
        "db.phase3.insert_one(d22)   \n",
        "db.phase3.insert_one(d23) "
      ],
      "metadata": {
        "id": "dDcgFO12-ZTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phase 4**"
      ],
      "metadata": {
        "id": "2WZG0eN9_Qut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d24 = {'Film': 'Black Widow', 'Release Date': '2021-07-09', 'Main Character': 'Black Widow', 'Articles and Tweets': np.nan}\n",
        "d25 = {'Film': 'Shang-Chi and the Legend of the Ten Rings', 'Release Date': '2021-09-03', 'Main Character': 'Shang-Chi', 'Articles and Tweets': np.nan}\n",
        "d26 = {'Film': 'Eternals', 'Release Date': '2021-11-05', 'Main Character': 'Eternals', 'Articles and Tweets': np.nan}\n",
        "d27 = {'Film': 'Spider-Man: No Way Home', 'Release Date': '2021-12-14', 'Main Character': 'Spider-Man', 'Articles and Tweets': np.nan}\n",
        "d28 = {'Film': 'Doctor Strange in the Multiverse of Madness', 'Release Date': '2022-05-06', 'Main Character': 'Doctor Strange', 'Articles and Tweets': np.nan}\n",
        " \n",
        "for film in diz_scraping:\n",
        "  if(d24['Film'] == film):\n",
        "    d24['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "  \n",
        "  elif(d25['Film'] == film):\n",
        "    d25['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "  \n",
        "  elif(d26['Film'] == film):\n",
        "    d26['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "  \n",
        "  elif(d27['Film'] == film):\n",
        "    d27['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "\n",
        "  elif(d28['Film'] == film):\n",
        "    d28['Articles and Tweets'] = diz_scraping[film]\n",
        "    print(film)\n",
        "\n",
        "# Salva documenti\n",
        "list_doc = [d24, d25, d26, d27, d28]\n",
        "for doc in list_doc:\n",
        "  with open('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/Mongodb DF (JSON)/Phase4:{}.json'.format(doc['Film']), 'w') as file:\n",
        "    json.dump(doc, file)\n",
        "\n",
        "# Carica su mongodb\n",
        "db.phase4.insert_one(d24)   \n",
        "db.phase4.insert_one(d25)   \n",
        "db.phase4.insert_one(d26)   \n",
        "db.phase4.insert_one(d27)   \n",
        "db.phase4.insert_one(d28)   "
      ],
      "metadata": {
        "id": "38Ii9_p92Afh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}