{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4)Quality Assessment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Controllo completezza Scraping"
      ],
      "metadata": {
        "id": "FpAnpNSGu3Is"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Quanti articoli tra quelli scaricati sono poi stati effettivamente utilizzati? Sono stati scartati quelli senza twitter_link"
      ],
      "metadata": {
        "id": "S_XCrcEcNRCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "\n",
        "# Colleghiamoci a Mydrive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Spostati nella cartella contenente tutti i dataset di scraping\n",
        "# path = '/content/drive/MyDrive/Dataset/JSON SCRAPING'  \n",
        "path1 = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON SCRAPING'\n",
        "path2 = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON SCRAPING CLEAN'\n",
        "\n",
        "os.chdir(path1)\n",
        "file_paths1 = [h for h in glob.glob(\"*.json\")]\n",
        "\n",
        "os.chdir(path2)\n",
        "file_paths2 = [i for i in glob.glob(\"*.json\")]\n",
        "\n",
        "for file_path in file_paths1:\n",
        "  # print(re.sub(r' Twitter scraping_news.json', '', file_path))\n",
        "  datas = {'Film' : [], 'Articoli trovati su Google news': [], \n",
        "            'Articoli utilizzati per scaricare tweets': [], 'Differenza': [],\n",
        "            \"Completezza rispetto all'attributo twitter_link (%)\": []}\n",
        "  try:\n",
        "    file_json1 = open(path1 + '/' + file_path)\n",
        "    file_json2 = open(path2 + '/' + re.sub(r\".json\", \"_clean.json\", file_path))\n",
        "    \n",
        "    data1 = json.load(file_json1)\n",
        "    data2 = json.load(file_json2)\n",
        "\n",
        "    datas['Film'].append(str(re.sub(r' Twitter scraping_news.json', '', file_path)))\n",
        "    datas['Articoli trovati su Google news'].append(int(len(data1)))\n",
        "    datas['Articoli utilizzati per scaricare tweets'].append(int(len(data2)))\n",
        "    datas['Differenza'].append(int(len(data1)-len(data2)))\n",
        "    datas[\"Completezza rispetto all'attributo twitter_link (%)\"].append(str(len(data2)/len(data1)*100))\n",
        "\n",
        "    a = pd.DataFrame(datas)\n",
        "    a.to_csv('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Tabelle Report/{}_completezza.csv'.format(file_path.strip(' Twitter scraping_news.json')))\n",
        "    file_json1.close()\n",
        "    file_json2.close()\n",
        "  except:\n",
        "    b = pd.DataFrame(columns=['Film', 'Articoli trovati su Google news', 'Articoli utilizzati per scaricare tweets', 'Differenza', \"Completezza rispetto all'attributo twitter_link (%)\"])\n",
        "    b['Film'] = file_path.strip(' Twitter scraping_news.json')\n",
        "    b.to_csv('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Tabelle Report/{}_completezza.csv'.format(file_path.strip(' Twitter scraping_news.json')))\n",
        "    continue\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gpOvrzPHqmuJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "path = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Tabelle Report'\n",
        "os.chdir(path)\n",
        "\n",
        "dir = [i for i in glob.glob(\"*_completezza.csv\")]\n",
        "\n",
        "a0 = pd.read_csv(path + '/' + dir[20], index_col = 'Unnamed: 0')\n",
        "for file in dir[1:]:\n",
        "  a1 = pd.read_csv(path + '/' + file, index_col = 'Unnamed: 0')\n",
        "  a0 = pd.concat([a0, a1], ignore_index=True)\n",
        "a0.to_csv('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Tab_quality_assessment/Tab_Completezza_Scraping.csv')\n",
        "a0"
      ],
      "metadata": {
        "id": "nVhkT6XYmHf-",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Controllo completezza Twitter"
      ],
      "metadata": {
        "id": "JI83qL6LuaJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Quanti tweet sono stati scaricati rispetto a quelli disponibili?"
      ],
      "metadata": {
        "id": "mAQAdHbbN0i5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "\n",
        "# Colleghiamoci a Mydrive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "sc = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON SCRAPING CLEAN'\n",
        "os.chdir(sc)\n",
        "\n",
        "file = [i for i in glob.glob(\"*.json\")]\n",
        "\n",
        "tw = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON TWITTER'\n",
        "os.chdir(tw)\n",
        "\n",
        "file2 = [h for h in glob.glob(\"*.json\")]\n",
        "\n",
        "data = {'Film': [], 'N° tweet_id estratti': [], 'N° tweet_id trovati': [], 'Differenza': [], 'Completezza in ipotesi di mondo aperto (rispetto a tutti i tweets estraibili)(%)': []}\n",
        "for f in file2:\n",
        "  data['Film'].append(re.sub(' Twitter.json', '', f).strip('Final_post_'))\n",
        "  \n",
        "  tw_film = json.loads(open(tw + '/' + f).read())\n",
        "  g = f.split('Final_post_')[1]\n",
        "  sc_film = json.loads(open(sc + '/' + re.sub('.json', ' scraping_news_clean.json', g)).read())\n",
        "  n_tw_sc = 0\n",
        "  for el in range(len(sc_film)):\n",
        "    n_tw_sc = n_tw_sc + len(sc_film[el]['twitter_id'])\n",
        "\n",
        "  data['N° tweet_id estratti'].append(n_tw_sc)  \n",
        "  data['N° tweet_id trovati'].append(len(tw_film))  \n",
        "  data['Differenza'].append(n_tw_sc-len(tw_film))  \n",
        "  data['Completezza in ipotesi di mondo aperto (rispetto a tutti i tweets estraibili)(%)'].append((len(tw_film)/n_tw_sc)*100)  \n",
        "\n",
        "a = pd.DataFrame(data) \n",
        "a.to_csv('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Tab_quality_assessment/Tab_Completezza_Mondo_Aperto.csv')\n",
        "a"
      ],
      "metadata": {
        "id": "29fKcveHMJ7d",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Controllo completezza attributi twitter"
      ],
      "metadata": {
        "id": "igKgOOX8WqD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Quanto sono completi i dataset contenenti i tweets?"
      ],
      "metadata": {
        "id": "7EUZrrpjN9V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "#Per ogni dataset, per ogni colonna guardo quanti sono le righe vuote / nulle fratto quelle totali\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import os\n",
        "import glob\n",
        "import math\n",
        "import json\n",
        "\n",
        "# Colleghiamoci a Mydrive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Spostati nella cartella contenente tutti i dataset di scraping\n",
        "path = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/3)TWEETS API'\n",
        "\n",
        "os.chdir(path)\n",
        "file_paths = [h for h in glob.glob(\"*.json\")]\n",
        "\n",
        "datas = {'film':[], 'hasthag_not_empty %': [], 'url_not_empty %': [], \n",
        "            'username_not_empty %': [], 'User_id_not_empty %': [],\n",
        "            \"User_verified_not_empty %\": [],\"User_creation_not_empty %\": [],\"Tweet_id_not_empty %\": [],\"Date_not_empty %\": [],\n",
        "           \"User_Description_not_empty %\": [],\"Text_not_empty %\": [],\"Source_not_empty %\": []}\n",
        "  \n",
        "for file_path in file_paths:\n",
        "  try:\n",
        "    file_json = open(path + '/' + file_path)\n",
        "    data = json.load(file_json)\n",
        "    count_hasthag_not_empty=0\n",
        "    count_url_not_empty=0\n",
        "    count_username_not_empty=0\n",
        "    count_User_id_not_empty=0\n",
        "    count_User_verified_not_empty=0\n",
        "    count_User_creation_not_empty=0\n",
        "    count_Tweet_id_not_empty=0\n",
        "    count_Date_not_empty=0\n",
        "    count_User_Description_not_empty=0\n",
        "    count_Text_not_empty=0\n",
        "    count_Source_not_empty=0\n",
        "    for tweet in data:\n",
        "      if len(tweet['Hashtags'])!=0:\n",
        "        count_hasthag_not_empty +=1\n",
        "      if len(tweet['Urls'])!=0:\n",
        "        count_url_not_empty +=1\n",
        "      if tweet['Username']!=\"\" and tweet['Username']!=None:\n",
        "        count_username_not_empty+=1\n",
        "      if tweet['User_id']!=\"\" and math.isnan(tweet['User_id'])==False:\n",
        "        count_User_id_not_empty+=1\n",
        "      if tweet['User_verified']!=\"\" and math.isnan(tweet['User_verified'])==False:\n",
        "        count_User_verified_not_empty+=1\n",
        "      if tweet['User_creation']!=\"\" and tweet['User_creation']!=None:\n",
        "        count_User_creation_not_empty+=1\n",
        "      if tweet['Tweet_id']!=\"\" and tweet['Tweet_id']!=None:\n",
        "        count_Tweet_id_not_empty+=1\n",
        "      if tweet['Date']!=\"\" and tweet['Date']!=None:\n",
        "        count_Date_not_empty+=1\n",
        "      if tweet['User_Description']!=\"\" and tweet['User_Description']!=None:\n",
        "        count_User_Description_not_empty+=1\n",
        "      if tweet['Text']!=\"\" and tweet['Text']!=None:\n",
        "        count_Text_not_empty+=1\n",
        "      if tweet['Source']!=\"\" and tweet['Source']!=None:\n",
        "        count_Source_not_empty+=1\n",
        "    \n",
        "    datas['film'].append(str(file_path.strip(' Twitter.json').strip('Final_post_')))\n",
        "    datas['hasthag_not_empty %'].append(str(count_hasthag_not_empty/len(data)*100))\n",
        "    datas['url_not_empty %'].append(str(count_url_not_empty/len(data)*100))\n",
        "    datas['username_not_empty %'].append(str(count_username_not_empty/len(data)*100))\n",
        "    datas['User_id_not_empty %'].append(str(count_User_id_not_empty/len(data)*100))\n",
        "    datas['User_verified_not_empty %'].append(str(count_User_verified_not_empty/len(data)*100))\n",
        "    datas['User_creation_not_empty %'].append(str(count_User_creation_not_empty/len(data)*100))\n",
        "    datas['Tweet_id_not_empty %'].append(str(count_Tweet_id_not_empty/len(data)*100))\n",
        "    datas['Date_not_empty %'].append(str(count_Date_not_empty/len(data)*100))\n",
        "    datas['User_Description_not_empty %'].append(str(count_User_Description_not_empty/len(data)*100))\n",
        "    datas['Text_not_empty %'].append(str(count_Text_not_empty/len(data)*100))\n",
        "    datas['Source_not_empty %'].append(str(count_Source_not_empty/len(data)*100))\n",
        "    \n",
        "  except:\n",
        "    bf = file_path.strip(' Twitter.json').strip('Final_post_')\n",
        "    datas['film'].append(bf)\n",
        "    datas['hasthag_not_empty %'].append('0')\n",
        "    datas['url_not_empty %'].append('0')\n",
        "    datas['username_not_empty %'].append('0')\n",
        "    datas['User_id_not_empty %'].append('0')\n",
        "    datas['User_verified_not_empty %'].append('0')\n",
        "    datas['User_creation_not_empty %'].append('0')\n",
        "    datas['Tweet_id_not_empty %'].append('0')\n",
        "    datas['Date_not_empty %'].append('0')\n",
        "    datas['User_Description_not_empty %'].append('0')\n",
        "    datas['Text_not_empty %'].append('0')\n",
        "    datas['Source_not_empty %'].append('0')\n",
        "    continue\n",
        "  \n",
        "a = pd.DataFrame(datas)\n",
        "a.to_csv('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Tab_quality_assessment/Tab_Completezza_Attributi_Tweets.csv')\n",
        "a.head(5)"
      ],
      "metadata": {
        "id": "CGt4Bo04xqXI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CONSISTENZA"
      ],
      "metadata": {
        "id": "2WY3atHyOBqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Controllo che data pubblicazione tweet (Date) deve essere successiva a data creazione utente (User_creation), altrimenti c'è un'inconsistenza in logica"
      ],
      "metadata": {
        "id": "kgPiNPgjiSj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# Le date di pubblicazione del tweet (twitter_id.Date) devonono essere precedenti alla data di pubblicazione dell'articolo (published)\n",
        " \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "\n",
        "# Colleghiamoci a Mydrive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/Mongodb DF (JSON)'\n",
        "os.chdir(path)\n",
        "\n",
        "file = [i for i in glob.glob(\"*.json\")]\n",
        "\n",
        "print('Film che presentano inconsistenza logica tra data creazione utente e data di pubblicazione del tweet')\n",
        "for f in file:\n",
        "  film = json.loads(open(path + '/' + f).read())\n",
        "## Controlliamo che Date >= User_creation \n",
        "  try:\n",
        "    for art_tw in film['Articles and Tweets']:\n",
        "      err_ut_tw = []\n",
        "      ut_date = []\n",
        "      tw_date = []\n",
        "      ut_err = []\n",
        "      err_ut_tw_count = 0\n",
        "      for tw in art_tw['twitter_id']:\n",
        "        anno_tw = int(tw['Date'].split('/')[2])\n",
        "        mese_tw = int(tw['Date'].split('/')[1])\n",
        "        giorno_tw = int(tw['Date'].split('/')[0])\n",
        "        \n",
        "        anno_ut = int(tw['User_creation'].split('/')[2])\n",
        "        mese_ut = int(tw['User_creation'].split('/')[1])\n",
        "        giorno_ut = int(tw['User_creation'].split('/')[0])\n",
        "        \n",
        "        # Caso 1: Utente e tweet differiscono per qualche giorno \n",
        "        if anno_tw == anno_ut:\n",
        "          if mese_tw == mese_ut:\n",
        "            if giorno_tw < giorno_ut:\n",
        "              # print('Errore: data creazione utente posteriore a data pubblicazione tweet')\n",
        "              err_ut_tw.append(tw['Tweet_id'])\n",
        "              ut_date.append(tw['User_creation'])\n",
        "              tw_date.append(tw['Date'])\n",
        "              ut_err.append(tw['User_id'])\n",
        "              err_ut_tw_count += 1\n",
        "            else:\n",
        "              # print('ok')\n",
        "              continue\n",
        "          else:\n",
        "            continue\n",
        "        else:\n",
        "          continue\n",
        "        \n",
        "        # Caso 2: Utente e tweet differiscono per qualche mese\n",
        "        if anno_tw == anno_ut:\n",
        "          if mese_tw < mese_ut:\n",
        "              # print('Errore: data creazione utente posteriore a data pubblicazione tweet')\n",
        "              err_ut_tw.append(tw['Tweet_id'])\n",
        "              ut_date.append(tw['User_creation'])\n",
        "              tw_date.append(tw['Date'])\n",
        "              ut_err.append(tw['User_id'])\n",
        "              err_ut_tw_count += 1\n",
        "          else:\n",
        "            continue\n",
        "        else:\n",
        "          continue\n",
        "\n",
        "        # Caso 3: Utente e tweet differiscono per qualche anno \n",
        "        if anno_tw < anno_ut:\n",
        "          err_ut_tw.append(tw['Tweet_id'])\n",
        "          ut_date.append(tw['User_creation'])\n",
        "          tw_date.append(tw['Date'])\n",
        "          ut_err.append(tw['User_id'])\n",
        "          err_ut_tw_count += 1\n",
        "        else:\n",
        "          continue\n",
        "\n",
        "    if err_ut_tw_count != 0: \n",
        "      print(re.sub('.json', '', f), 'n° inconsistenze utente-tweet = ', err_ut_tw_count, '; Tweet_id inconsistenti = ', err_ut_tw, \n",
        "            '; User_id inconsistenti = ', ut_err, 'Data creazione utente:', ut_date, 'Data pubblicazione tweet:', tw_date)\n",
        "    else:\n",
        "      print(re.sub('.json', '', f), 'Documento consistente')\n",
        "  except:\n",
        "    # print('Articles and Tweets empty')\n",
        "    continue\n"
      ],
      "metadata": {
        "id": "vXCHXCBY37ck",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Controllo data pubblicazione tweet (Date) deve essere precedente a data pubblicazione articolo (published), altrimenti c'è un'inconsistenza in logica"
      ],
      "metadata": {
        "id": "mNY6ADW1iunP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "\n",
        "# Colleghiamoci a Mydrive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/Mongodb DF (JSON)'\n",
        "os.chdir(path)\n",
        "\n",
        "file = [i for i in glob.glob(\"*.json\")]\n",
        "\n",
        "print('Film che presentano inconsistenza logica tra data di pubblicazione del tweet e data pubblicazione articolo')\n",
        "\n",
        "for f in file:\n",
        "  film = json.loads(open(path + '/' + f).read())\n",
        "## Controlliamo che Date <= published \n",
        "  try:\n",
        "  # if film['Articles and Tweets'] == film['Articles and Tweets']:\n",
        "    data = {'Film': [], 'art_date': [], 'art_date' : [], 'err_art_tw' : [], 'tw_date' : []}\n",
        "\n",
        "    for art_tw in film['Articles and Tweets']:\n",
        "      anno_art = int(art_tw['published'].split('/')[2])\n",
        "      mese_art = int(art_tw['published'].split('/')[1])\n",
        "      giorno_art = int(art_tw['published'].split('/')[0])\n",
        "        \n",
        "      for tw in art_tw['twitter_id']:\n",
        "        anno_tw = int(tw['Date'].split('/')[2])\n",
        "        mese_tw = int(tw['Date'].split('/')[1])\n",
        "        giorno_tw = int(tw['Date'].split('/')[0])\n",
        "\n",
        "        if anno_art == anno_tw:\n",
        "          if mese_art == mese_tw:\n",
        "            if giorno_art < giorno_tw:\n",
        "              data['Film'].append(f.strip('.json'))\n",
        "              data['err_art_tw'].append(tw['Tweet_id'])\n",
        "              data['art_date'].append(art_tw['published'])\n",
        "              data['tw_date'].append(tw['Date'])\n",
        "            else:\n",
        "              continue    \n",
        "          if mese_art < mese_tw:\n",
        "            data['Film'].append(f.strip('.json'))\n",
        "            data['err_art_tw'].append(tw['Tweet_id'])\n",
        "            data['art_date'].append(art_tw['published'])\n",
        "            data['tw_date'].append(tw['Date'])\n",
        "          else:\n",
        "            # print('ok')\n",
        "            continue\n",
        "        if anno_art < anno_tw:\n",
        "          data['Film'].append(f.strip('.json'))\n",
        "          data['err_art_tw'].append(tw['Tweet_id'])\n",
        "          data['art_date'].append(art_tw['published'])\n",
        "          data['tw_date'].append(tw['Date'])\n",
        "        else:\n",
        "          # print('ok2')\n",
        "          continue\n",
        "      \n",
        "    if len(data) != 0:\n",
        "      a = pd.DataFrame(data)\n",
        "      a.to_csv('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Tabelle Report/{}x.csv'.format(f.strip('.json')))\n",
        "      # print(a)\n",
        "    else:\n",
        "      b = pd.DataFrame(columns=['Film', 'art_date', 'err_art_tw', 'tw_date'])\n",
        "      b['Film'] = f.strip('.json')\n",
        "      b.to_csv('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Tabelle Report/{}x.csv'.format(f.strip('.json')))\n",
        "      # print(f.strip('.json'), 'Documento consistente')\n",
        "  except:\n",
        "    c = pd.DataFrame(columns=['Film', 'art_date', 'err_art_tw', 'tw_date'])\n",
        "    c['Film'] = f.strip('.json')\n",
        "    c.to_csv('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Tabelle Report/{}x.csv'.format(f.strip('.json')))\n",
        "    continue"
      ],
      "metadata": {
        "id": "B01FdQ_cqB57",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "path = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Tabelle Report'\n",
        "os.chdir(path)\n",
        "\n",
        "dir = [i for i in glob.glob(\"*x.csv\")]\n",
        "\n",
        "a0 = pd.read_csv(path + '/' + dir[0], index_col = 'Unnamed: 0')\n",
        "for file in dir[1:]:\n",
        "  a1 = pd.read_csv(path + '/' + file, index_col = 'Unnamed: 0')\n",
        "  a0 = pd.concat([a0, a1], ignore_index=True)\n",
        "\n",
        "a0.rename(columns = {'art_date': 'Data pubblicazione articolo', 'err_art_tw': 'Tweet_id', 'tw_date': 'Data pubblicazione tweet'}, inplace = True)\n",
        "a0.to_csv('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Tab_quality_assessment/Tab_Consistenza_tra_DataArticolo_DataTweet.csv')\n",
        "a0.groupby('Film', as_index = False).count().rename(columns={'Tweet_id': 'N° inconsistenze'})[['Film', 'N° inconsistenze']]"
      ],
      "metadata": {
        "id": "-jRpjgMxM3MY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "a0.head(5)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "76V3GzQsPOAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Controllo consistenza per unicità tweet id"
      ],
      "metadata": {
        "id": "OyvqtZPIb5Bs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Controlliamo che non ci siano tweet_id duplicati"
      ],
      "metadata": {
        "id": "QGUuJMQdWcLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import os\n",
        "import glob\n",
        "import math\n",
        "import json\n",
        "\n",
        "# Colleghiamoci a Mydrive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Spostati nella cartella contenente tutti i dataset di scraping\n",
        "path = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/4)DOCUMENTS MONGODB'\n",
        "\n",
        "os.chdir(path)\n",
        "file_paths = [h for h in glob.glob(\"*.json\")]\n",
        "\n",
        "df = {'Film': [], 'N° Inconsistenze': [], 'Totale': [], 'Rapporto N° Inconsistenze/Totale(%)': []}\n",
        "\n",
        "for file_path in file_paths:\n",
        "  file_json = open(path + '/' + file_path)\n",
        "  data = json.load(file_json)\n",
        "  diz_id_user={}\n",
        "  list_tweet_id =[]\n",
        "  count_duplicate_tweet_id=0\n",
        "  count_inconsistency=0\n",
        "  count_tot=0\n",
        "  if data['Articles and Tweets']==data['Articles and Tweets']:\n",
        "    if tweet['Tweet_id'] not in diz_id_user:\n",
        "      diz_id_user[tweet['Tweet_id']]=tweet['Username']\n",
        "    else:\n",
        "      if diz_id_user[tweet['Tweet_id']]!=tweet['Username']:\n",
        "        count_inconsistency+=1\n",
        "    count_tot+=1\n",
        "  \n",
        "  df['Film'].append(re.sub('.json', '', file_path).strip('Final_post_'))\n",
        "  df['N° Inconsistenze'].append(count_inconsistency)\n",
        "  df['Totale'].append(count_tot)\n",
        "  try:\n",
        "    df['Rapporto N° Inconsistenze/Totale(%)'].append(str(count_inconsistency/count_tot*100))\n",
        "  except:\n",
        "    df['Rapporto N° Inconsistenze/Totale(%)'].append(np.nan)\n",
        "    continue\n",
        "\n",
        "data = pd.DataFrame(df)\n",
        "data.to_csv('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Tab_quality_assessment/Unicità_Tweet_id.csv')\n",
        "a"
      ],
      "metadata": {
        "id": "sW3Ib4spjReJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}